---
layout: post
title: "《设计数据密集型应用》 第三部分 记录系统和衍生数据系统"
date:   2025-2-5
tags: [设计数据密集型应用, 一致性算法]
comments: true
author: huanghuoguoguo
---

在本书的 [第一部分](https://ddia.vonng.com/#/part-i) 和 [第二部分](https://ddia.vonng.com/#/part-ii) 中，自底向上地把所有关于分布式数据库的主要考量都过了一遍。从数据在磁盘上的布局，一直到出现故障时分布式系统一致性的局限。但所有的讨论都假定了应用中只用了一种数据库。

现实世界中的数据系统往往更为复杂。大型应用程序经常需要以多种方式访问和处理数据，没有一个数据库可以同时满足所有这些不同的需求。因此应用程序通常组合使用多种组件：数据存储，索引，缓存，分析系统，等等，并实现在这些组件中移动数据的机制。

本书的最后一部分，会研究将多个不同数据系统（可能有着不同数据模型，并针对不同的访问模式进行优化）集成为一个协调一致的应用架构时，会遇到的问题。软件供应商经常会忽略这一方面的生态建设，并声称他们的产品能够满足你的所有需求。在现实世界中，集成不同的系统是实际应用中最重要的事情之一。

## [记录系统和衍生数据系统](https://ddia.vonng.com/#/part-iii?id=记录系统和衍生数据系统)

从高层次上看，存储和处理数据的系统可以分为两大类：

- 记录系统（System of record）

记录系统，也被称为 真相源（source of truth），持有数据的权威版本。当新的数据进入时（例如，用户输入）首先会记录在这里。每个事实正正好好表示一次（表示通常是 正规化的，即 normalized）。如果其他系统和 记录系统 之间存在任何差异，那么记录系统中的值是正确的（根据定义）。

- 衍生数据系统（Derived data systems）

衍生系统 中的数据，通常是另一个系统中的现有数据以某种方式进行转换或处理的结果。如果丢失衍生数据，可以从原始来源重新创建。典型的例子是 缓存（cache）：如果数据在缓存中，就可以由缓存提供服务；如果缓存不包含所需数据，则降级由底层数据库提供。非规范化的值，索引和物化视图亦属此类。在推荐系统中，预测汇总数据通常衍生自用户日志。

从技术上讲，衍生数据是 冗余的（redundant），因为它重复了已有的信息。但是衍生数据对于获得良好的只读查询性能通常是至关重要的。它通常是非规范化的。可以从单个源头衍生出多个不同的数据集，使你能从不同的 “视角” 洞察数据。

并不是所有的系统都在其架构中明确区分 记录系统 和 衍生数据系统，但是这是一种有用的区分方式，因为它明确了系统中的数据流：系统的哪一部分具有哪些输入和哪些输出，以及它们如何相互依赖。

大多数数据库，存储引擎和查询语言，本质上既不是记录系统也不是衍生系统。数据库只是一个工具：如何使用它取决于你自己。记录系统和衍生数据系统之间的区别不在于工具，而在于应用程序中的使用方式。

通过梳理数据的衍生关系，可以清楚地理解一个令人困惑的系统架构。这将贯穿本书的这一部分。

## [章节概述](https://ddia.vonng.com/#/part-iii?id=章节概述)

我们将从 [第十章](https://ddia.vonng.com/#/ch10) 开始，研究例如 MapReduce 这样 面向批处理（batch-oriented） 的数据流系统。对于建设大规模数据系统，我们将看到，它们提供了优秀的工具和思想。[第十一章](https://ddia.vonng.com/#/ch11) 将把这些思想应用到 流式数据（data streams） 中，使我们能用更低的延迟完成同样的任务。[第十二章](https://ddia.vonng.com/#/ch12) 将对本书进行总结，探讨如何使用这些工具来构建可靠，可伸缩和可维护的应用。

## 第十章 批处理

### 使用Unix工具的批处理

Unix工具如`grep`、`awk`、`sed`等在批处理中发挥着重要作用。它们可以快速处理文本文件，过滤、转换和分析数据。例如，使用`grep`可以快速搜索日志文件中特定的错误信息，使用`awk`可以对日志中的字段进行统计和分析。

#### 简单日志分析

日志分析是批处理中的一个常见任务。通过解析日志文件，可以提取关键信息，如错误率、响应时间等。例如，使用`awk`可以快速计算出每分钟的请求次数：

bash**复制**

```bash
awk '{print $4}' access.log | awk -F: '{print $2}' | sort | uniq -c
```

#### Unix哲学

Unix哲学强调“做一件事，并做好”。每个工具专注于一个任务，通过管道`|`将多个工具组合起来，完成复杂的任务。例如，`cat`、`grep`、`awk`、`sort`等工具可以组合起来完成日志分析任务。

### MapReduce和分布式文件系统

MapReduce是一种编程模型，用于处理大规模数据集。它将任务分解为Map和Reduce两个阶段，Map阶段对数据进行分片和处理，Reduce阶段对中间结果进行汇总。分布式文件系统如HDFS提供了高可用性和容错性，确保数据在集群中的可靠存储。

#### MapReduce作业执行

MapReduce作业的执行包括以下几个步骤：

1. **输入分片**：将输入数据分成多个分片。
2. **Map阶段**：对每个分片的数据进行处理，生成中间键值对。
3. **Shuffle阶段**：将Map阶段的输出进行排序和分区，发送给Reduce任务。
4. **Reduce阶段**：对中间键值对进行汇总和处理，生成最终结果。

#### Reduce侧连接与分组

在MapReduce中，Reduce侧连接通过将不同数据源的数据发送到同一个Reduce任务中进行处理，实现数据的连接和分组。例如，可以将用户信息和交易信息发送到同一个Reduce任务中，计算每个用户的总交易额。

#### Map侧连接

Map侧连接在Map阶段完成数据的连接，减少了网络传输和Shuffle阶段的开销。例如，可以将用户信息和交易信息在Map阶段进行连接，生成中间键值对。

#### 批处理工作流的输出

批处理工作流的输出通常需要经过多次转换和聚合。例如，可以将 logs 数据转换为 CSV 格式，便于进一步分析和可视化。

#### Hadoop与分布式数据库的对比

Hadoop和分布式数据库各有优缺点。Hadoop适合处理大规模数据集，但不适合频繁的随机读写。分布式数据库如Cassandra适合处理频繁的随机读写，但扩展性可能不如Hadoop。

### MapReduce之后

MapReduce之后，出现了许多新的批处理框架，如Spark。Spark提供了更高效的内存计算和更丰富的API，适用于更复杂的数据处理任务。

#### 物化中间状态

在批处理中，物化中间状态可以提高效率。例如，在迭代处理中，将中间结果存储在分布式缓存中，避免重复计算。

#### 图与迭代处理

图处理是批处理中的一个重要领域。例如，使用Pregel模型可以处理大规模的图数据，如社交网络分析。

#### 高级API和语言

高级API和语言如Spark SQL、Hive等提供了更简洁的批处理方式。例如，可以使用SQL语句在Hive中进行数据查询和分析。

### 本章小结

批处理是处理大规模数据集的重要技术。通过使用Unix工具、MapReduce、分布式文件系统和高级API，可以高效地处理和分析数据。了解批处理的原理和工具，可以更好地应对大规模数据处理的挑战。

## 第十一章：流处理

流处理和批处理类似，只不过前者是无边界或者边界极短的数据范围，批处理是事先确定好的数据范围。

### 传递事件流

事件流是指在分布式系统中传递的事件序列，这些事件通常表示系统中的状态变化或用户行为。传递事件流是流处理的基础，通过将事件从源头传递到处理系统，可以实现实时数据处理和分析。

#### 消息传递系统

消息传递系统是实现事件流传递的关键组件。常见的消息传递系统包括Kafka、RabbitMQ和ActiveMQ。这些系统提供了高吞吐量、低延迟和高可用性的消息传递服务，确保事件能够可靠地从源头传递到处理系统。

#### 分区日志

分区日志是Kafka等消息传递系统中的一种数据结构，用于将日志数据划分为多个分区，从而实现并行处理和高吞吐量。每个分区可以独立地进行读写操作，提高了系统的并发能力和可扩展性。

### 数据库与流

数据库与流处理的结合可以实现数据的实时存储和查询。例如，可以将流处理的结果存储到数据库中，以便后续的分析和查询。同时，数据库也可以作为流处理的源头，提供实时的数据更新。

#### 保持系统同步

在分布式系统中，保持系统同步是确保数据一致性的关键。通过使用消息传递系统和事件溯源等技术，可以实现系统之间的同步。例如，当一个系统中的数据发生变化时，可以通过事件流将这些变化传递到其他系统，确保所有系统的数据保持一致。

#### 变更数据捕获

变更数据捕获（CDC）是一种用于捕获数据库中数据变化的技术。通过CDC，可以将数据库中的增量变化实时传递到流处理系统中，从而实现数据的实时处理和分析。例如，当数据库中的表发生插入、更新或删除操作时，CDC可以捕获这些变化并将其作为事件传递到Kafka等消息传递系统中。

#### 事件溯源

事件溯源是一种将系统状态变化记录为事件序列的技术。通过事件溯源，可以实现系统的可追溯性和可恢复性。例如，当系统发生故障时，可以通过重放事件序列来恢复系统的状态。

#### 状态、流和不变性

在流处理中，状态是指系统在处理事件时所维护的内部数据。流是指事件的序列，不变性是指系统在处理事件时所保持的某些属性。例如，可以通过维护一个计数器来记录事件的数量，这个计数器就是状态，事件序列就是流，计数器的值始终是非负的，这就是不变性。

### 流处理

流处理是指对事件流进行实时处理和分析的过程。流处理系统可以对事件流进行过滤、转换、聚合等操作，从而提取有用的信息。例如，可以使用Apache Flink或Apache Storm等流处理框架来实现流处理。

#### 流处理的应用

流处理在许多领域都有广泛的应用，例如：

- **金融**：实时监控交易风险，检测异常交易行为。
- **物联网**：实时处理传感器数据，监控设备状态。
- **社交媒体**：实时分析用户行为，推荐相关内容。

#### 时间推理

时间推理是指在流处理中处理时间相关的问题。例如，可以通过时间窗口来对事件进行分组和聚合，从而实现对时间序列数据的分析。例如，可以使用滑动窗口来计算每分钟的事件数量。

#### 流连接

流连接是指将多个事件流进行连接和关联的操作。例如，可以将用户行为流和商品信息流进行连接，从而实现个性化推荐。流连接可以通过多种方式进行，例如基于时间窗口的连接、基于键值的连接等。

#### 容错

容错是指在流处理中处理系统故障的能力。流处理系统需要具备容错机制，以确保在系统发生故障时，能够继续处理事件流。例如，可以通过检查点和重放机制来实现容错。当系统发生故障时，可以从最近的检查点重新开始处理事件流，从而确保数据的一致性和完整性。

### 本章小结

流处理是处理实时数据的重要技术，通过使用消息传递系统、事件溯源、状态管理等技术，可以实现对事件流的实时处理和分析。流处理在金融、物联网、社交媒体等领域有广泛的应用，可以用于实时监控、分析和决策。了解流处理的原理和应用，可以更好地应对实时数据处理的挑战。





## 第十二章：数据系统的未来

这章没怎么看。

### 数据集成

#### 组合使用衍生数据的工具

数据集成涉及将来自不同来源的数据组合在一起，以提供更全面的视图。衍生数据是指从原始数据中提取或计算得到的数据。组合使用衍生数据的工具可以帮助我们更好地理解和利用数据。例如，可以使用ETL（Extract, Transform, Load）工具将不同数据源的数据提取出来，进行清洗和转换，然后加载到数据仓库中进行分析。

#### 批处理与流处理

批处理和流处理是两种常见的数据处理方式。批处理是指对静态数据进行处理，通常用于处理大量历史数据。流处理是指对实时数据进行处理，通常用于处理实时事件流。例如，可以使用批处理来分析昨天的销售数据，使用流处理来实时监控今天的销售情况。

#### 铁路上的模式迁移

铁路上的模式迁移是指在铁路系统中，将传统的数据处理模式迁移到新的数据处理模式。例如，可以将传统的批处理模式迁移到流处理模式，以实现实时监控和分析。这种迁移可以提高系统的响应速度和效率。

### 分拆数据库

分拆数据库是指将一个大型数据库拆分成多个小型数据库，以提高系统的可扩展性和性能。例如，可以将一个包含所有用户数据的大型数据库拆分成多个小型数据库，每个小型数据库只包含一部分用户的数据。这样可以减少单个数据库的负载，提高系统的响应速度。

#### 组合使用数据存储技术

组合使用数据存储技术是指根据不同的数据类型和访问模式，选择合适的数据存储技术。例如，可以使用关系型数据库存储结构化数据，使用NoSQL数据库存储非结构化数据，使用数据湖存储大量原始数据。通过组合使用不同的数据存储技术，可以更好地满足不同的数据存储和访问需求。

#### 围绕数据流设计应用

围绕数据流设计应用是指根据数据的流动和处理需求，设计应用程序。例如，可以设计一个实时监控系统，将传感器数据流实时传输到处理系统，进行分析和决策。这种设计可以提高系统的实时性和响应速度。

#### 观察衍生数据状态

观察衍生数据状态是指监控和分析衍生数据的变化，以了解系统的运行状态。例如，可以监控衍生数据的生成速度、存储量和访问频率，以评估系统的性能和资源使用情况。这种观察可以帮助我们及时发现和解决潜在的问题。

### 将事情做正确

将事情做正确是指在数据处理和分析过程中，确保数据的准确性和可靠性。例如，可以通过数据清洗、数据验证和数据审计等方法，确保数据的质量。这种做法可以提高数据的可信度，支持更准确的决策。

#### 数据库的端到端原则

数据库的端到端原则是指从数据的生成、存储、处理到分析，整个过程都应该遵循一致的原则和标准。例如，可以确保数据在生成时就符合一定的格式和规范，在存储和处理过程中保持数据的一致性和完整性。这种原则可以提高数据的可用性和可靠性。

#### 强制约束

强制约束是指在数据处理和分析过程中，强制执行某些规则和限制。例如，可以强制要求数据必须经过清洗和验证才能存储，或者强制要求分析结果必须符合一定的标准。这种约束可以提高数据的质量和可靠性。

#### 及时性与完整性

及时性与完整性是指在数据处理和分析过程中，既要确保数据的及时性，又要确保数据的完整性。例如，可以使用实时数据处理技术来确保数据的及时性，同时使用数据验证和数据审计来确保数据的完整性。这种平衡可以提高数据的可用性和可靠性。

#### 信任但验证

信任但验证是指在数据处理和分析过程中，既要信任数据源，又要对数据进行验证。例如，可以信任传感器数据，但同时对数据进行清洗和验证，以确保数据的准确性和可靠性。这种做法可以提高数据的可信度，支持更准确的决策。

### 做正确的事情

做正确的事情是指在数据处理和分析过程中，始终遵循正确的原则和方法。例如，可以确保数据的生成、存储、处理和分析都符合一定的标准和规范，避免数据的误用和滥用。这种做法可以提高数据的可用性和可靠性，支持更准确的决策。

#### 预测性分析

预测性分析是指使用数据和模型来预测未来的趋势和结果。例如，可以使用历史销售数据和机器学习模型来预测未来的销售趋势，帮助企业制定更有效的营销策略。这种分析可以提高企业的竞争力和市场响应能力。

#### 隐私和追踪

隐私和追踪是指在数据处理和分析过程中，既要保护用户的隐私，又要对数据的使用进行追踪和审计。例如，可以使用加密和匿名化技术来保护用户的隐私，同时使用日志和审计技术来追踪数据的使用情况。这种做法可以提高数据的安全性和合规性。

### 本章小结

数据集成是现代数据处理的重要组成部分，涉及将来自不同来源的数据组合在一起，以提供更全面的视图。通过组合使用衍生数据的工具、批处理与流处理、铁路上的模式迁移、分拆数据库、组合使用数据存储技术、围绕数据流设计应用、观察衍生数据状态、将事情做正确、数据库的端到端原则、强制约束、及时性与完整性、信任但验证、做正确的事情、预测性分析、隐私和追踪等方法，可以更好地管理和利用数据，支持更准确的决策和更高效的业务运营。