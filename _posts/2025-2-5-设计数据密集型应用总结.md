---
layout: post
title: "设计数据密集型应用 观后感"
date:   2025-2-5
tags: [设计数据密集型应用, 一致性算法]
comments: true
author: huanghuoguoguo
---

从1月16号开，到2月4号完，历时两个多礼拜。粗读了该书的内容。想读得仔细一些也办不到，很多概念和方法没有经过实践和探索，光靠目前接触和掌握的技术栈和项目，只能吃个大概。现在慢慢梳理一下每章节的内容，以及和自己掌握的知识做一个联系。以前看redis设计与实现的时候，根本不清楚当时redis选用这样的策略是为了解决什么问题，直到自己也动手开发一个具有分布式性质的应用或者组件的时候，思考过诸多解决方法，才会学习到。

在开始看这本书的时候，发现和自己之前接触的暑期分布式存储项目的java管理侧，OceanBase数据库有很大的联系，其中很多概念提前接触了一部分，但是都没有学习系统的知识。又由于自己做了一个demo是关于任务调度引擎的，面临什么问题，怎么解决的，按照什么思想解决的，都是模糊的，所以对这本书有很大的兴趣。换做一年前的我，是绝对不会碰这本书的。我已经犯过很多次超前学习的错误，这本书的部分内容很适合现在的我阅读。

### 第一部分：数据系统的基石

在阅读一本书的开始，我首先期望带着我当前认知水平的问题去看。我思考从一个简单的kv map开始，怎么一步一步增强其功能，变成一个持久化的，高可用的，可维护的数据库。思考一个产品的迭代路程对学习非常有帮助，因为这让人理解技术的用武之地，而不是生搬硬套。

程序的基石是数据，数据从哪里来，到哪里去，是什么种类，怎么存储**更符合业务逻辑和需求？**

在这一章学习了很多数据的表示方式，存储方式，以及著名的开源中间件采用了什么方式，为什么这个方式更符合他们的程序设计，以提供了非常不错的性能等等。同时还学习了有哪些范式、规则能够让数据设计的时候有前向兼容性和后向兼容性，虽然我都记不住了。

# 最简单的数据库

什么是世界上最简单的数据库？只需要两个bash函数：

```plain
1
2
3
4
5
6
7

#!/bin/bash
db_set () {
    echo "$1,$2" >> database
}
db_get () {
    grep "^$1," database | sed -e "s/^$1,//" | tail -n 1
}
```

使用例子：

```plain
1
2
3
4
5
6
7
8
9
10
11

$ db_set 123456 '{"name":"London","attractions":["Big Ben","London Eye"]}' 
$ db_set 42 '{"name":"San Francisco","attractions":["Golden Gate Bridge"]}'
$ db_get 42
{"name":"San Francisco","attractions":["Golden Gate Bridge"]}
$ db_set 42 '{"name":"San Francisco","attractions":["Exploratorium"]}' 
$ db_get 42
{"name":"San Francisco","attractions":["Exploratorium"]}
$ cat database
123456,{"name":"London","attractions":["Big Ben","London Eye"]} 
42,{"name":"San Francisco","attractions":["Golden Gate Bridge"]} 
42,{"name":"San Francisco","attractions":["Exploratorium"]}
```

这个最简单的数据库底层存储文件是一个文本文件”database”，每行是用逗号分隔的key和value。每次调用db_set，把key和value追加到dababase文件中。每次调用db_get，查询的是对应key的最后一次写入记录。

这个数据库具备了存储和查询这两种最基本的数据库功能。存储使用了append操作，而不是随机写。对于写操作，append是最快的操作。对于读操作，使用的是线性查找，复杂度O(n)。随着数据量的增加，查找所花的时间也会线性增长。

为了提升查询性能，可以给数据增加索引。索引可以显著地提升查询性能，但同时也会降低写入性能，因为写入时除了要写入数据本身，还要更新索引，所以需要权衡如何高效地建立索引。

# 哈希索引

一种直观地建立索引的方法是使用哈希表索引(Hash Indexes)。我们可以在内存中建立一个哈希表，哈希表的key是数据的key，哈希表的value是这个key对应的数据所在的文件的偏移。举个例子，上文123456和42这两个key的索引如下：

![img](https://cdn.nlark.com/yuque/0/2025/png/32754462/1738729735893-0ec80cf8-3069-4d1b-86c3-2a9dc6b7152b.png)

[Bitcask存储引擎](https://riak.com/assets/bitcask-intro.pdf)使用的就是这种方式。Bitcask存储引擎写入时对文件执行的是追加(append)操作，查询的时候通过建立哈希索引加快查询性能。哈希表保存在内存中。这种存储引擎比较适合key的数量有限，并且写操作比较频繁的场景。Bitcask存储引擎遇到的问题和解决方案如下。

**数据文件只有append操作，磁盘早晚会有耗尽的一天，怎么解决这个问题？**

解决方案是写入数据的时候，把数据文件分成固定的大小，一个数据文件达到大小后，写入新的文件中。然后对已经写入的文件进行压缩(compaction)操作。因为同一个key的操作可能有很多次，只有最后一次操作的值才有意义，之前操作的记录没有存在的必要，所以我们可以遍历已经保存过的文件，只保留每个key的最后一次操作值，然后把这些最新的值写入新的数据文件中，老的数据文件就可以删除了。压缩完了还需要更新哈希表中key对应的文件偏移。

![img](https://cdn.nlark.com/yuque/0/2025/png/32754462/1738729755679-0b94a5d7-dd00-4c9e-b0dd-0620d059904b.png)

上图中yawn, scratch, memw, purr这四个key在老的数据文件中出现了多次，经过压缩后，数据减少了很多。

**哈希表只保存在内存中，进程重启或者崩溃的时候怎么办？**

一种解决方法是进程重启的时候，扫描所有的数据文件，在内存中重建哈希表。这个过程可能很耗时间，所以Bitcask使用的优化方式是把哈希表也保存在磁盘中，重启的时候可以加载磁盘的哈希表，快速建立索引。当然这样会降低写入的性能，因为写入数据的时候需要把哈希索引表也写入磁盘中，需要权衡。

哈希索引也有缺点，主要有两个：

1. 如果key的数量很多，内存不够大，无法把所有的key都保存在内存中。
2. 范围查询效率不高。比如需要查询kitty00000到kitty99999这个范围的数据，必须先到哈希表中查询每个key的文件偏移位置，再去读磁盘。

# SSTable和LSM-Tree

既然哈希索引范围查询效率不高，有没有优化方法呢？

有一种解决方案是使用Sorted String Table(SSTable)。上文介绍Bitcask存储引擎的数据分成了一个个固定大小的文件，每个文件中key的顺序是不固定的。SSTable的区别在于每个数据文件是根据key排过序的。这是如何做到的呢？

首先，数据写入的时候先把数据保存在内存中的平衡树结构中（比如红黑树或者AVL树）。这种内存树结构也被叫做memtable。

当memtable的大小超过一定值时（比如几MB），把当前memtable写入磁盘的SSTable文件中。因为平衡树本身是有序的，所以把它们写入磁盘的时候也可以保持顺序写入。新的写请求会写入到内存中新的memtable。

和Bitcask存储引擎一样，SSTables方案里有一个后台压缩进程持续地对已经生成的SSTable文件进行合并。合并的方式可以采用归并排序(mergesort)算法，这样合并后的SSTable文件也是有序的。排序过程中，如果一个key在多个文件中出现，只需要使用最新的文件中的数据。如下图所示：

![img](https://cdn.nlark.com/yuque/0/2025/png/32754462/1738729752510-af39a09a-8755-4b1b-870f-a1b6e0c1eade.png)

使用SSTables结构，查询数据时，首先到内存memtable中查询数据是否存在，如果不存在，再到磁盘的SSTable文件中查找。对于SSTable文件，内存中也有key的索引表，区别在于我们不需要在内存中保存所有的key。因为SSTable中数据是有序的，我们只需要保存少数几个key的文件位置偏移就行了。如下图所示：

![img](https://cdn.nlark.com/yuque/0/2025/png/32754462/1738729746005-178cd1cc-5169-4426-aeda-aaece0b0eb20.png)

对于key的范围为handbag和handprinted，内存中只需要保存handbag的位置偏移，查找key时先查找和这个key最相近的key所对应的位置偏移，然后读取整个偏移范围的内容，对这个范围内的数据再查找我们想要查的key。以handiwork为例，查找这个key时，先在内存索引表中找到最近的key为handbag，然后从SSTable文件中对应偏移位置开始读取对应长度的数据，然后扫描这段范围的数据，找到handiwork的值。一般来说内存索引表中一个key对应几KB的数据，所以扫描很快。另外保存这段范围的数据时，可以压缩之后再写入磁盘，这样增加磁盘IO的效率，不过这也带来了CPU压缩和解压缩的开销。比如handbag到handprinted这个范围的数据可以先压缩后再保存在磁盘中。

上文说到查询数据时先到memtable中查找key是否存在，不存在再到最新的SSTable文件中查找，然后再到旧的SSTable文件中查找，一直到最老的SSTable文件。如果数据库中根本不存在这个key的话，查找过程可能会比较耗时间，一种优化方式是使用[布隆过滤器(Bloom filter)](https://en.wikipedia.org/wiki/Bloom_filter)来判断key是否不存在，key存在的话再执行上面的查找流程。

SSTable方案内存中保存了memtable，如果进程重启或者崩溃怎么处理？一种方案是内存写入memtable前，磁盘先写入一个log文件，这个log文件只有append操作。进程重启的时候，使用这个log文件重建memtable。每次内存中的memtable写入SSTable文件后，这个log文件也可以删除了。

整个方案写入数据的时候写入memtable和append-only log文件中，所以写入性能很高，同时SSTable文件是有序的，进行范围查询的时候性能也很高。

方案最早是在[The Log-Structured Merge-Tree (LSM-Tree)](https://www.cs.umb.edu/~poneil/lsmtree.pdf)这篇论文中描述，所以也叫LSM-Tree。很多数据库都使用了LSM-Tree，包括LevelDB、RocksDB、Cassandra、HBase等。

# B-Tree

虽然LSM-Tree近来发展很快，B-Tree仍然是目前使用最广泛的索引结构。

B-Tree也是一种有序的数据结构，可以高效地进行范围查找。示例如下：

![img](https://cdn.nlark.com/yuque/0/2025/png/32754462/1738729743232-bad33d0c-80f6-466a-ae4a-96626fd1902f.png)

B-Tree中，每个数据文件的大小是固定，一般为4K或者更大，称作块(block)或者页(page)。B-Tree是分层结构。最上层的页作为根页。根页中按顺序保存了一些key，同时保存了指向子页的指针，也就是子页数据文件的位置。每个子页保存了一段范围的key和对应的位置指针。最底层的称做叶子页(leaf page)，叶子页保存了所有的数据，也有实现方式是在叶子页中只保存数据的指针。每页中保存的指针数量叫做分支系数(branching factor)，一般是几百个。大部分数据库B-Tree的层数是3到4层。对于4层的B-Tree，如果页是4KB，分支系数是500的话，可以储存256TB数据。

B-Tree添加key的时候，需要找到包含这个key的页，如果这个页已经满了，需要分裂成两个新的页。示例如下：

![img](https://cdn.nlark.com/yuque/0/2025/png/32754462/1738729738258-6252221a-be52-4e06-b2d2-a2c5bdd70a52.png)

B-Tree修改页的时候也面临进程崩溃的问题。为了解决这个问题，写入的时候会先写入一个append-only log文件中，也叫做write-ahead log(WAL)或者[redo log](https://yang.observer/2020/05/06/distributed-transaction/#redo日志)。WAL写入成功后再写入数据页，如果进程崩溃了，WAL可以用来恢复数据页。

# B-Tree和LSM-Tree比较

我们来比较一下B-Tree和LSM-Tree。一般来说，LST-Tree写入速度更快，而B-Tree查询速度更快。LSM-Tree查询慢一些的原因是因为需要查询几次才能在对应的SSTable中找到数据。不过究竟哪个更合适还是需要根据自己业务数据的性能测试结果来定。

B-Tree写入时需要至少两次IO操作，第一次写入WAL，第二次写入数据页。LSM-Tree执行写入操作的时候只需要写入一次WAL，另外一次是内存操作。但是因为后台有个压缩进程会反复对数据进行合并和压缩，一个数据会反复写入多次，这对SSD不太友好，因为SSD的写入次数是有限的。另外后台压缩进程运行的时候会占据大量磁盘IO，可能会影响正在执行的写入操作。随着数据文件的增大，压缩占据的磁盘IO也会更多，如果同时写入操作也很多的话，有可能压缩操作的速度赶不上写入操作的速度，导致磁盘空间被用光。

LSM-Tree更容易被压缩，所以数据文件比B-Tree要小一些。而且B-Tree数据页可能不是完全利用的，比如一个页分裂成两个页的时候，这两个页都只有一半空间被使用了，LSM-Tree不存在这个问题。

# 二级索引

我们知道很多数据库都可以建立二级索引，它的实现方式和一级索引一样，也可以使用B-Tree或者LSM-Tree。这里会遇到的问题主要是数据本身存在哪里？

一种方式是数据存在每个索引中，这样会造成数据重复，有几个索引就会有几份数据，更新数据的时候会比较麻烦，需要更新多份数据，可能会导致一致性问题。这种方式也叫聚集索引(clustered index)，MySQL InnoDB的主键使用了这种方式。

另外一种方式是数据只有一份，所有索引中不保存数据，只保存数据的指针。更新数据的时候如果数据长度没有变大的话，只需要更新一次。如果数据变大的话，可能需要把数据保存在新的地方，所有索引中的数据指针都需要更新。MySQL InnoDB中的二级索引使用了类似的方式，没有保存数据，保存的是主键中的数据指针。

还有一种折中方案是在索引文件中只保存部分数据列，而不是整行，这也称作覆盖索引(covering index)。





文档型数据库和记录型数据库各有优劣，随着多模态数据大火，文档型数据库也越来越流行。但是程序都是为人服务的，怎么选型，需要各个组件配合使用。

一个数据库应当做到两件基础的事情：当你把数据交给数据库时，它应当存起来；当你需要这部分数据时，它应当还给你。为了实现这一目标，数据库的存储引擎设计至关重要。研究两类存储引擎：日志结构的存储引擎和面向页面的存储引擎。kafka是典型的日志结构，OceanBase也是日志结构，他们考虑AP和TP不同的侧重，充分利用存储介质的特性。innodb是典型的面向页面的存储引擎。

学习了LSM树和SStable，MMemtable，SStable加上合并就可以说是LSM树了，曙光的底层kvdb就是一个LSM树的数据库。重要的是学习其思想，他还是利用了局部性原理：查询很可能只查找最近的key，如此一来将最近的key存储在内存中，大幅提高了写性能，不必像面向页面的数据库那样频繁的换入换出。通过异步合并不可变的块，也能减少追加形式的日志存储大小。

经常看到的面试题，用较小的内存处理大数据，就可以用这两种方式：1. 数据压缩，用bit替代int。2. swap，将数据分段放在磁盘，或者本身就是从磁盘读入的，分段处理。用map reduce的思想，归并处理。lsm树也可以认为是归并。

第四章讨论了encode，decode，序列化，网络传输等。对此我的理解不是很深。
所以综合来说，这一部分讨论了一个数据密集型的应用需要从哪部分往上建立。

### 第二部分： 分布式数据

这一部分是我最感兴趣的，因为之前对redis复制和分片是什么有疑惑。不太清除哨兵机制是作用于什么，主从复制和分片怎么协同，或者是我以为这两个是一个东西。在这一部分可以得到大部分解答。

说到分布式就不得不提到共识。之前了解过的共识算法主要是zab，raft，kraft，paxos，等等。他们都有的共同点是，需要解决谁是当前集群中最大最新的（term），然后再决定谁是下一代领导者。决定下一个共识操作怎么被接受。

在之前决定做从曙光偷来的调度job引擎的时候我就有考虑过，因为我看到xxl-job使用http来进行调度器和执行器之间的交互，我知道必须加上这一项。所以我就考虑了几个轻量级的raft实现框架，但是才疏学浅，没能参透那些状态机和日志的使用方式，最终采用了依靠redis stream的方式，用轻量级的mq实现节点间的相互通信。这好像不相关，为什么实现节点间相互通信需要raft？用http，tcp不行吗？好像是这样的啊。所以最终我用redis分布式锁实现选主，这样最简单。对外依赖最少。后续用etcd替代。redis用goosip协议传播元数据，分片情况下一定是不可靠的。这个就不考虑先了。

回到本书当中来，复制。

复制(Replication)是通过网络把一份数据保存在多个机器上。为什么需要复制数据？原因主要有三个：

1. 把数据放在离用户近的位置，减少网络延迟。
2. 某台机器崩溃后，其他具有相同数据的机器可以继续服务，增加可用性。
3. 把读请求分散在不停机器上，增加读吞吐量。

通常有三种复制方案：

- 主从复制(Leaders and Followers)
- 多主复制(Multi-Leader Replication)
- 无主复制(Leaderless Replication)

最先接触到的当然是redis的主从复制，读写分离，redis fork子进程传输rdb给子节点。增量同步，全量同步，缓冲区同步，等等。全量同步之后，就会用同步二进制日志的形式进行同步。还有一个很重要的概念叫做复制缓冲区。redis为什么要引入哨兵机制？直接用raft内置不好吗？

Redis广泛应用于缓存场景，对数据一致性的要求相对较低，更注重性能和可用性。哨兵机制在这些场景下能够提供足够的高可用性支持。要从应用场景出发，弱化了redis的一致性要求。

学习了单主复制，多主复制，无主复制。他们的典型：

- 单主复制，主节点负责写。保证没有写冲突。

- redis

- 多主复制，多个主节点可以同时写入。

- 这里其实我觉得非常模糊。似乎没有绝对的多主。像redis，gpt给出的回答是他也算，因为在分片情况下有多个主从分布，他们也可以算多领导者，但是他们的写是不冲突的呀，因为分片不一样。
- kafka。

- 无主复制，没有明确的主节点。

- kafka，多个node有多个分区，每个分区有多个复制冗余在其他节点上，并且某个node上的分区是主分区接受写入。这其实不也算是单领导者吗？因为写入是在单分区上写入的。

https://yang.observer/2022/02/26/ddia-replication/?utm_source=chatgpt.com

之前一直有个疑问就是，多主下怎么解决写冲突。这里提供了几个方法。

处理冲突的一种方式是避免冲突(Conflict avoidance)。比如不同ID的写请求路由到不同的leader上。但是这样可能失去了多主复制的优势，使用多主的目的是为了提升可用性或者降低延迟，如果把数据分配在不同leader上，如果某个leader崩溃，多主复制就没有用了。

另外一种方式是一致性状态收敛(converging toward a consistent state)。在主从复制中，写操作是顺序发生的，不会产生冲突。多主复制中，由于写操作可能同时发生，需要解决冲突，让数据收敛到一致的状态。常用的方案有：

- last write wins(LWW)，给每个写请求分配唯一ID，发生冲突时，保留ID最大的写操作，丢弃其他并发写操作。
- 给每个副本分配唯一ID，发生冲突时，保留ID最大的副本上的写操作，丢弃其他副本的写操作。
- 同时写入冲突的数据，并且记录冲突状态，之后由用户来解决冲突(比如弹窗让用户选择数据)。

![img](https://cdn.nlark.com/yuque/0/2025/png/32754462/1738729550730-62af260d-ee4d-4b55-b8b2-52c581f87f39.png)

如图所示，客户端写入时，把请求同时发给3个副本，收到2个副本的成功响应，第3个副本故障了不影响这次的请求，只要有多数派返回成功就认为这次写入成功。那第3个副本重启后，缺少了这次写请求的数据，怎么同步这次数据的改动？有两种办法。

- 读修复：客户端读数据的时候，向所有副本发请求，如上图所示，这时副本1和副本2返回了最新的数据，版本号是7，而副本3返回的是旧数据，版本号是6，这时客户端向副本3发一次写请求，更新副本3上的数据。
- 后台进程修复：有个后台进程定时扫描副本之间数据的差异，同步最新的数据。这个方案的好处是可以同步所有数据，读修复方案只能修复热数据，冷数据可能要很久才有机会修复。

选择写或者读的成功数量时，有个条件: `w + r > n`，其中n为总副本个数，w为写请求至少收到成功响应的个数，r为读请求至少收到成功响应的个数。只要满足`w + r > n`就可保证一定能读到最新的数据。一般n会设置成奇数，`w = r = (n + 1) / 2`，这样可以容忍 n / 2 个副本故障。根据业务的情况，可以修改w和r。比如对于写很少，读很多的业务，可以设置`w = n, r = 1`，不过这样不能容忍写入时有副本故障。

无主复制适合对可用性和低延迟要求比较高，对一致性或者读到旧数据要求不高的场景。

接下来是分区。

最先接触到的当然也是redis hash分区，16328 crc分区。

要么hash，要么二级索引，像12306那个项目的分库分表，就是用id路由。还有一致性哈希。

分区后，客户端怎么知道数据在哪个分区，怎么把请求路由到正确的分区？有三种方式，如下图所示。

![img](https://cdn.nlark.com/yuque/0/2025/png/32754462/1738729687707-c5c0cadb-2898-4ccd-a9b9-52a199509b1e.png)

1. 客户端随机找个节点，每个节点保存了当前的分区信息。如果这个节点刚好有对应数据，直接返回客户端，如果没有，把请求转发给对应的节点。
2. 客户端把请求发给负载均衡服务器，这个服务器知道分区信息，转发请求给对应的节点。
3. 客户端知道当前的分区信息，发请求到对应的服务器。

其中最大的挑战在于**如何让每个节点都知道分区信息的变化，让所有节点产生共识(consensus)**。一种方式是将分区信息放在ZooKeep上。但是如何实现？再平衡过程中如何保持服务的正常运行？如何保证数据迁移前后的一致性？**数据迁移**是一个比较麻烦的问题，有时间再专门研究。

第七章 事务

原子性，一致性，持久性，隔离性。

四个隔离级别。

mysql mvcc+临键锁可以解决大部分幻读问题。

弱隔离级别是指在事务处理过程中，允许一定程度的数据不一致性的隔离级别。常见的弱隔离级别包括读未提交（Read Uncommitted）、读已提交（Read Committed）和可重复读（Repeatable Read）等。

可串行化是指事务的执行顺序可以被重新排列，以确保数据的一致性。可串行化是事务处理中的一个重要概念，它可以确保即使在高并发的情况下，系统的数据一致性也能得到保证。

可串行化（Serializable）：可串行化是最高级别的隔离级别，它要求事务的执行顺序必须与串行执行的顺序相同。这可以确保数据的一致性，但可能会导致性能问题。

可串行化并不是数据库以串行化的方式执行任务，而是保证能够以串行化执行的结果**并发**执行事务。

第八章 分布式系统的麻烦

在分布式情况下，想要保证可用性的同时还期望一致性，需要解决几个问题。

节点故障，时钟同步不准确。

第九章 一致性与共识

在分布式系统中，一致性保证是确保所有节点数据一致性的关键。一致性保证可以分为强一致性、弱一致性和最终一致性等不同类型。

**强一致性**：强一致性要求所有节点的数据在任何时刻都保持一致。这通常通过同步更新和全局锁来实现，但在分布式系统中实现成本较高。

**弱一致性**：弱一致性允许在某些情况下数据不一致，但在一定时间内会最终达到一致。这通常通过异步更新和版本控制来实现，适用于对实时性要求不高的场景。

**最终一致性**：最终一致性允许在某些情况下数据不一致，但最终会在一定时间内达到一致。这通常通过异步更新和并发控制来实现，适用于对实时性要求较低的场景。

还有线性一致性。

第三部分讨论了流处理和批处理，在之后的内容没有看了，完全摸不着头脑，和现在的知识水平不匹配，等以后接触了更多再来看。

实习

很感谢去年暑期在曙光的实习，阅读了很多实践的代码，包括分布式一致性的内存级别的缓存，当时不理解的很多东西现在读了书之后也慢慢理解了。

# 对比与总结

| **技术栈**    | **复制方式**                                                 | **分片方式**                                                 |
| ------------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| **MySQL**     | 主从复制：通过二进制日志同步数据，主节点记录日志，从节点应用日志。半同步复制：主节点等待至少一个从节点确认后再提交事务。Galera 协议：多节点同步复制，基于 MVCC。 | 表分区：原生支持，但分区在单个服务器上。分库分表：通过中间件（如 ShardingSphere、Vitess）实现水平扩展。但是目前mysql的原生分区并不流行。至少我没怎么听说过，只听说过分库分表。**缺点****限制较多**：分区键通常需要是主键的一部分，且对某些 DDL 操作支持有限。**管理复杂性**：分区表在数据量较大时，执行 DDL、备份和恢复等操作可能变得复杂所以当单表数据量增大时，优先考虑的是分库分表。 |
| **Redis**     | 主从复制：从节点定期同步主节点数据，支持全量和增量同步。哨兵模式：监控主从复制状态，自动故障切换。这里值得开一个大章节，这里简化。 | 分片集群：基于哈希槽（16384个槽）分配数据，通过 CRC16 算法计算槽位。这里值得开一个大章节，这里简化。 |
| **RocketMQ**  | DLedger 复制：基于 Raft 算法，支持分布式一致性。             | 消息队列分片：按 Topic 分配消息队列，队列分布在不同 Broker 上。 |
| **RabbitMQ**  | 镜像队列：消息在多个节点间同步，支持高可用。分片队列：基于 Raft 算法，支持 Leader 选举和故障恢复。 | 分片队列：通过 Raft 算法实现队列分片。                       |
| **Kafka**     | 分区复制：以分区为单位，配置一主多从，通过 ISR（In-Sync Replicas）确保副本一致性。 | 分区策略：支持轮询、基于消息哈希或自定义策略分配消息到分区。 |
| **OceanBase** | 分布式事务复制：支持多副本强一致性，采用 Paxos 协议。        | 数据分片：将大表划分为多个小分片（tablet），分片由多个节点管理。 |

## mysql和redis的对比

### 对比与总结

| **特性**       | **MySQL 主从架构 + 分库分表**            | **Redis 哨兵模式 + 分片架构**              |
| -------------- | ---------------------------------------- | ------------------------------------------ |
| **数据一致性** | 主从节点数据一致，分片之间数据独立       | 哨兵模式主从一致，分片架构通过哈希槽分片   |
| **高可用性**   | 主从架构支持手动或工具辅助的故障切换     | 哨兵模式自动故障转移，分片架构支持自动切换 |
| **扩展性**     | 分库分表支持水平扩展，每个分片可独立扩展 | 分片架构支持水平扩展，通过增加主从节点     |
| **读写分离**   | 主从架构支持读写分离                     | 分片架构支持读写分离，主从节点分担负载     |
| **复杂性**     | 需要中间件或应用层支持分库分表           | 哨兵和分片架构由 Redis 内置支持            |

总的来说

- **MySQL 的主从架构与分库分表**：适用于需要高可用性和水平扩展的场景，但需要额外的中间件或应用层支持。
- **Redis 的哨兵模式与分片架构**：哨兵模式专注于主从架构的高可用性，而分片架构通过哈希槽实现分布式存储和自动故障转移。
